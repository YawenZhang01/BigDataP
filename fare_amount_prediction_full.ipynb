{"cells": [{"cell_type": "markdown", "id": "54422731-1ed3-4328-9a37-a64d46122722", "metadata": {}, "source": "# Business Problem 1: Hourly Fare Amount Prediction\n"}, {"cell_type": "code", "execution_count": 1, "id": "53b8d8eb-f36a-446a-824c-98cb68f679fb", "metadata": {}, "outputs": [], "source": "from pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import lit\nfrom pyspark.sql.functions import col, lower, trim, date_format, dayofweek, month, hour, minute, datediff, count, avg, max, min, to_date, to_timestamp, sum, when\nfrom pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\nfrom pyspark.ml.regression import RandomForestRegressor\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np"}, {"cell_type": "code", "execution_count": 2, "id": "844dc3b4-84fd-4704-af53-37d3c4589f02", "metadata": {}, "outputs": [], "source": "spark = SparkSession.builder.appName('FullPredict')\\\n        .config(\"spark.executor.cores\",'16')\\\n        .config(\"spark.executor.memory\", '60g')\\\n        .getOrCreate()"}, {"cell_type": "markdown", "id": "5cda76bb-cb01-4253-bfab-6d3a68308936", "metadata": {}, "source": "## Data Processing"}, {"cell_type": "code", "execution_count": 3, "id": "10c0339e-b2b6-43a2-b65e-3875da740243", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# load data\ndf2021 = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"gs://project_hyrn/2021_Yellow_Taxi_Trip_Data_full.csv\")\ndf2020 =spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"gs://project_hynr2/2020_Yellow_Taxi_Trip_Data_full.csv\")\ndf2019 =spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"gs://project_hynr2/2019_Yellow_Taxi_Trip_Data_full.csv\")"}, {"cell_type": "code", "execution_count": 4, "id": "25caa661-32a7-4aad-8de0-95394babe447", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- VendorID: integer (nullable = true)\n |-- tpep_pickup_datetime: string (nullable = true)\n |-- tpep_dropoff_datetime: string (nullable = true)\n |-- passenger_count: integer (nullable = true)\n |-- trip_distance: double (nullable = true)\n |-- RatecodeID: integer (nullable = true)\n |-- store_and_fwd_flag: string (nullable = true)\n |-- PULocationID: integer (nullable = true)\n |-- DOLocationID: integer (nullable = true)\n |-- payment_type: integer (nullable = true)\n |-- fare_amount: double (nullable = true)\n |-- extra: double (nullable = true)\n |-- mta_tax: double (nullable = true)\n |-- tip_amount: double (nullable = true)\n |-- tolls_amount: double (nullable = true)\n |-- improvement_surcharge: double (nullable = true)\n |-- total_amount: double (nullable = true)\n |-- congestion_surcharge: double (nullable = true)\n |-- Datayear: integer (nullable = false)\n\n"}], "source": "df2021 = df2021.withColumn(\"Datayear\", lit(2021))\ndf2020 = df2020.withColumn(\"Datayear\", lit(2020))\ndf2019 = df2019.withColumn(\"Datayear\", lit(2019))\n\n# merge data\nfull_df = df2021.union(df2020).union(df2019)\nfull_df.printSchema()"}, {"cell_type": "code", "execution_count": 5, "id": "4315da0d-14d9-4bba-be0f-58de8266b432", "metadata": {}, "outputs": [], "source": "# partition\n\n#display number of records by partition\ndef displaypartitions(df):\n    #number of records by partition\n    num = df.rdd.getNumPartitions()\n    print(\"Partitions:\", num)\n    df.withColumn(\"partitionId\", F.spark_partition_id()).groupBy(\"partitionId\").count()\\\n        .orderBy(F.asc(\"count\"))\\\n        .show(num)"}, {"cell_type": "code", "execution_count": 6, "id": "916b22d5-f2cb-423b-8934-b3029a81752b", "metadata": {}, "outputs": [{"data": {"text/plain": "103"}, "execution_count": 6, "metadata": {}, "output_type": "execute_result"}], "source": "#number of partitions\nfull_df.rdd.getNumPartitions()"}, {"cell_type": "code", "execution_count": 7, "id": "17be62ad-d43f-41cd-bd16-2fa20435fa82", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 6:======================================================>(102 + 1) / 103]\r"}, {"name": "stdout", "output_type": "stream", "text": "Partitions: 64\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 12:================================>                    (123 + 12) / 200]\r"}, {"name": "stdout", "output_type": "stream", "text": "+-----------+-------+\n|partitionId|  count|\n+-----------+-------+\n|         46|2186696|\n|         45|2186698|\n|         44|2186700|\n|         43|2186702|\n|         42|2186705|\n|         41|2186706|\n|         40|2186708|\n|         39|2186710|\n|         38|2186711|\n|         36|2186713|\n|         37|2186713|\n|         35|2186713|\n|         34|2186713|\n|         33|2186714|\n|         32|2186715|\n|         31|2186716|\n|         30|2186717|\n|         29|2186718|\n|         28|2186721|\n|         27|2186725|\n|         26|2186726|\n|         25|2186727|\n|         24|2186729|\n|         23|2186730|\n|         22|2186731|\n|         20|2186735|\n|         21|2186735|\n|         19|2186736|\n|         18|2186737|\n|         17|2186738|\n|         16|2186738|\n|         15|2186740|\n|         14|2186742|\n|         13|2186743|\n|         11|2186745|\n|         12|2186745|\n|         10|2186747|\n|          8|2186748|\n|          9|2186748|\n|          7|2186749|\n|          6|2186751|\n|          5|2186753|\n|          4|2186754|\n|          3|2186757|\n|          2|2186762|\n|          1|2186763|\n|          0|2186765|\n|         63|2186766|\n|         62|2186768|\n|         61|2186771|\n|         60|2186773|\n|         59|2186776|\n|         58|2186777|\n|         57|2186778|\n|         56|2186779|\n|         55|2186784|\n|         54|2186785|\n|         52|2186786|\n|         53|2186786|\n|         51|2186790|\n|         50|2186793|\n|         49|2186795|\n|         48|2186797|\n|         47|2186798|\n+-----------+-------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "#re-partition the data to evenly distributed across 103 partitions\nfull_df = full_df.repartition(64)\ndisplaypartitions(full_df)"}, {"cell_type": "code", "execution_count": 8, "id": "3bd033d8-4563-4e8a-9479-5c834a0613a1", "metadata": {}, "outputs": [], "source": "# change to date type\nfull_df = full_df.withColumn(\"parsed_pickup_datetime\", to_timestamp(\"tpep_pickup_datetime\", \"MM/dd/yyyy hh:mm:ss a\"))\nfull_df = full_df.withColumn(\"parsed_dropoff_datetime\", to_timestamp(\"tpep_dropoff_datetime\", \"MM/dd/yyyy hh:mm:ss a\"))\n\n# Extract the hour, month, day from pickup time\nfull_df = full_df.withColumn(\"pickup_hour\", hour(\"parsed_pickup_datetime\"))\nfull_df = full_df.withColumn(\"pickup_month\", month(\"parsed_pickup_datetime\"))\nfull_df = full_df.withColumn(\"pickup_dayofweek\", dayofweek(\"parsed_pickup_datetime\"))"}, {"cell_type": "code", "execution_count": 9, "id": "ea82d6ed-31ff-4a38-bb4d-3278a2348a12", "metadata": {}, "outputs": [], "source": "# Calculate trip time\nfull_df = full_df.withColumn(\"Dur\", datediff(\"parsed_dropoff_datetime\", \"parsed_pickup_datetime\") * 24 * 60 +hour(\"parsed_dropoff_datetime\")* 60-hour(\"parsed_pickup_datetime\")*60\n+ minute(\"parsed_dropoff_datetime\") - minute(\"parsed_pickup_datetime\"))"}, {"cell_type": "code", "execution_count": 10, "id": "2ce02255-8b4e-466a-bbf1-77d25a4fd793", "metadata": {}, "outputs": [], "source": "# Calculate speed\nfull_df = full_df.withColumn(\"speed\", col(\"Dur\")/col(\"trip_distance\"))"}, {"cell_type": "code", "execution_count": 11, "id": "6e6322b3-8730-44cd-879c-3e2e932811ee", "metadata": {}, "outputs": [], "source": "# Create a dummy variable of weekday\nfull_df = full_df.withColumn(\"pickup_weekday\", when((full_df[\"pickup_dayofweek\"] >= 2) & (full_df[\"pickup_dayofweek\"] <= 6), 1).otherwise(0))"}, {"cell_type": "code", "execution_count": 12, "id": "f5b6f40c-9702-42c3-825d-3076f031fcd3", "metadata": {}, "outputs": [], "source": "# Create a season variable\nfull_df = full_df.withColumn(\n    \"season\",\n    when((full_df.pickup_month <= 2) | (full_df.pickup_month == 12), 1)\n    .when((full_df.pickup_month >= 3) & (full_df.pickup_month <= 5), 2)\n    .when((full_df.pickup_month >= 6) & (full_df.pickup_month <= 8), 3)\n    .otherwise(4),\n)"}, {"cell_type": "markdown", "id": "d32bd1c2-c1af-4ba2-b89e-2aaa74a40821", "metadata": {}, "source": "Helena: According to the dustribution of trip distances (very right skewed) and the fact that it takes about 30 miles to drive across the whole New York City, we decided to use 30 as the number to split the trips into short or long distance trips.\nWill need to adjust threshold when using FULL Datase"}, {"cell_type": "code", "execution_count": 13, "id": "f3933e62-2964-4556-aea9-4bc7c5aa1ead", "metadata": {}, "outputs": [], "source": "# Create a dummy variable of long_trip\nfull_df = full_df.withColumn(\"long_trip\", when(full_df[\"trip_distance\"] >= 30, 1).otherwise(0))"}, {"cell_type": "code", "execution_count": 14, "id": "120aeff2-99df-4be2-9a9f-57db2bfdf576", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- VendorID: integer (nullable = true)\n |-- tpep_pickup_datetime: string (nullable = true)\n |-- tpep_dropoff_datetime: string (nullable = true)\n |-- passenger_count: integer (nullable = true)\n |-- trip_distance: double (nullable = true)\n |-- RatecodeID: integer (nullable = true)\n |-- store_and_fwd_flag: string (nullable = true)\n |-- PULocationID: integer (nullable = true)\n |-- DOLocationID: integer (nullable = true)\n |-- payment_type: integer (nullable = true)\n |-- fare_amount: double (nullable = true)\n |-- extra: double (nullable = true)\n |-- mta_tax: double (nullable = true)\n |-- tip_amount: double (nullable = true)\n |-- tolls_amount: double (nullable = true)\n |-- improvement_surcharge: double (nullable = true)\n |-- total_amount: double (nullable = true)\n |-- congestion_surcharge: double (nullable = true)\n |-- Datayear: integer (nullable = false)\n |-- parsed_pickup_datetime: timestamp (nullable = true)\n |-- parsed_dropoff_datetime: timestamp (nullable = true)\n |-- pickup_hour: integer (nullable = true)\n |-- pickup_month: integer (nullable = true)\n |-- pickup_dayofweek: integer (nullable = true)\n |-- Dur: integer (nullable = true)\n |-- speed: double (nullable = true)\n |-- pickup_weekday: integer (nullable = false)\n |-- season: integer (nullable = false)\n |-- long_trip: integer (nullable = false)\n\n"}], "source": "full_df.printSchema()"}, {"cell_type": "code", "execution_count": 15, "id": "d3103f24-36d2-4d63-a985-0e13684f827b", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/05/18 22:07:19 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n[Stage 13:=====================================================>(102 + 1) / 103]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+--------+----------------------+-----------------------+-----------+------------+----------------+---+-----------------+--------------+------+---------+\n|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Datayear|parsed_pickup_datetime|parsed_dropoff_datetime|pickup_hour|pickup_month|pickup_dayofweek|Dur|            speed|pickup_weekday|season|long_trip|\n+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+--------+----------------------+-----------------------+-----------+------------+----------------+---+-----------------+--------------+------+---------+\n|       1|10/24/2019 05:00:...| 10/24/2019 05:10:...|              2|          1.5|         1|                 N|         236|         141|           1|        8.5|  3.5|    0.5|      2.55|         0.0|                  0.3|       15.35|                 2.5|    2019|   2019-10-24 17:00:15|    2019-10-24 17:10:42|         17|          10|               5| 10|6.666666666666667|             1|     4|        0|\n|       1|10/26/2019 10:56:...| 10/26/2019 11:15:...|              1|          3.2|         1|                 N|         100|         236|           2|       14.5|  2.5|    0.5|       0.0|         0.0|                  0.3|        17.8|                 2.5|    2019|   2019-10-26 10:56:03|    2019-10-26 11:15:41|         10|          10|               7| 19|           5.9375|             0|     4|        0|\n|       2|10/23/2019 06:53:...| 10/23/2019 07:08:...|              2|         3.06|         1|                 N|         238|          74|           2|       12.5|  1.0|    0.5|       0.0|         0.0|                  0.3|        16.8|                 2.5|    2019|   2019-10-23 18:53:53|    2019-10-23 19:08:58|         18|          10|               4| 15|4.901960784313726|             1|     4|        0|\n|       2|10/25/2019 05:18:...| 10/25/2019 05:41:...|              4|         2.53|         1|                 N|         166|          75|           1|       15.5|  1.0|    0.5|      3.46|         0.0|                  0.3|       20.76|                 0.0|    2019|   2019-10-25 17:18:33|    2019-10-25 17:41:04|         17|          10|               6| 23|9.090909090909092|             1|     4|        0|\n|       2|10/25/2019 11:30:...| 10/25/2019 11:53:...|              1|         3.64|         1|                 N|         142|         107|           1|       16.5|  0.5|    0.5|      5.08|         0.0|                  0.3|       25.38|                 2.5|    2019|   2019-10-25 23:30:37|    2019-10-25 23:53:16|         23|          10|               6| 23|6.318681318681318|             1|     4|        0|\n|       2|10/26/2019 10:24:...| 10/26/2019 10:31:...|              1|         0.88|         1|                 N|         163|         143|           2|        6.0|  0.5|    0.5|       0.0|         0.0|                  0.3|         9.8|                 2.5|    2019|   2019-10-26 22:24:57|    2019-10-26 22:31:10|         22|          10|               7|  7|7.954545454545454|             0|     4|        0|\n|       2|10/22/2019 10:22:...| 10/22/2019 10:26:...|              1|          0.5|         1|                 N|         162|         161|           1|        4.5|  0.5|    0.5|      1.66|         0.0|                  0.3|        9.96|                 2.5|    2019|   2019-10-22 22:22:40|    2019-10-22 22:26:12|         22|          10|               3|  4|              8.0|             1|     4|        0|\n|       2|10/26/2019 07:05:...| 10/26/2019 07:10:...|              1|         1.25|         1|                 N|           7|           7|           1|        6.0|  0.0|    0.5|      1.86|         0.0|                  0.3|       11.16|                 2.5|    2019|   2019-10-26 19:05:46|    2019-10-26 19:10:41|         19|          10|               7|  5|              4.0|             0|     4|        0|\n|       2|10/24/2019 08:34:...| 10/24/2019 08:44:...|              1|         1.22|         1|                 N|         163|         142|           1|        8.0|  0.5|    0.5|      1.77|         0.0|                  0.3|       13.57|                 2.5|    2019|   2019-10-24 20:34:24|    2019-10-24 20:44:55|         20|          10|               5| 10| 8.19672131147541|             1|     4|        0|\n|       2|10/22/2019 09:41:...| 10/22/2019 09:46:...|              2|         1.63|         1|                 N|         239|          24|           1|        6.5|  0.0|    0.5|       2.0|         0.0|                  0.3|        11.8|                 2.5|    2019|   2019-10-22 09:41:58|    2019-10-22 09:46:58|          9|          10|               3|  5|3.067484662576687|             1|     4|        0|\n+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+--------+----------------------+-----------------------+-----------+------------+----------------+---+-----------------+--------------+------+---------+\nonly showing top 10 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "full_df.show(10)"}, {"cell_type": "code", "execution_count": 15, "id": "15785f78-7e1f-4fc8-80c8-f34f483d3a82", "metadata": {}, "outputs": [], "source": "# Filter for fare_amount >=3.0, starting fare\nfull_df = full_df.filter(condition = col(\"fare_amount\") >= 3.0)\n"}, {"cell_type": "code", "execution_count": 16, "id": "a02883e5-36ed-4fe6-b936-98344be7c693", "metadata": {}, "outputs": [], "source": "# Filter out extreme points\nfull_df = full_df.filter((col(\"trip_distance\") >= 0) & (col(\"trip_distance\") < 500) & (col(\"fare_amount\") < 400))"}, {"cell_type": "code", "execution_count": 17, "id": "decf4940-dfb0-4682-9b97-90da28fc99ef", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 15:======================================================> (62 + 2) / 64]\r"}, {"name": "stdout", "output_type": "stream", "text": "+-------+------------------+-----------------+------------------+\n|summary|          Datayear|     pickup_month|  pickup_dayofweek|\n+-------+------------------+-----------------+------------------+\n|  count|         138867891|        138867891|         138867891|\n|   mean| 2019.616798018485|6.255678211459264| 4.129873694128472|\n| stddev|0.8229403454606475|3.620464463384807|1.9425838227962384|\n|    min|              2019|                1|                 1|\n|    max|              2021|               12|                 7|\n+-------+------------------+-----------------+------------------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Check date variables: Looks good\nfull_df.describe('Datayear', 'pickup_month','pickup_dayofweek').show()"}, {"cell_type": "code", "execution_count": 22, "id": "5644781e-0a20-40bb-9fac-2f29f28d3c8b", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 27:============================================>          (52 + 12) / 64]\r"}, {"name": "stdout", "output_type": "stream", "text": "+-------------------+\n|missing_fare_amount|\n+-------------------+\n|                  0|\n+-------------------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Check for missing values in a fare column: No missing values\nfull_df.select([count(when(col(\"fare_amount\").isNull(), \"fare_amount\")).alias(\"missing_fare_amount\")]).show()"}, {"cell_type": "code", "execution_count": 18, "id": "c39bae2e-c73a-4395-b78f-c08b58bccb42", "metadata": {}, "outputs": [], "source": "# log fare amount, tip amount, trip distance\nfrom pyspark.sql.functions import log1p\n\n# Add a new column with log-transformed fare_amount values\nfull_df = full_df.withColumn(\"lfare_amount\", log1p(full_df[\"fare_amount\"]))\nfull_df = full_df.withColumn(\"ltip_amount\", log1p(full_df[\"tip_amount\"]))\nfull_df = full_df.withColumn(\"ltrip_distance\", log1p(full_df[\"trip_distance\"]))\n\n# Drop the original fare_amount column\n# full_df = full_df.drop(\"fare_amount\",\"tip_amount\",\"trip_distance\")"}, {"cell_type": "code", "execution_count": 19, "id": "82a24167-3a2f-4e53-8f4d-202827849bcf", "metadata": {}, "outputs": [], "source": "# Drop useless variables\nfull_df = full_df.drop('total_amount', 'tpep_pickup_datetime','tpep_dropoff_datetime', 'parsed_pickup_datetime', 'parsed_dropoff_datetime')\n"}, {"cell_type": "code", "execution_count": 20, "id": "4bc6fbdf-37d7-42c1-bd6e-151af573ef07", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- VendorID: integer (nullable = true)\n |-- passenger_count: integer (nullable = true)\n |-- trip_distance: double (nullable = true)\n |-- RatecodeID: integer (nullable = true)\n |-- store_and_fwd_flag: string (nullable = true)\n |-- PULocationID: integer (nullable = true)\n |-- DOLocationID: integer (nullable = true)\n |-- payment_type: integer (nullable = true)\n |-- fare_amount: double (nullable = true)\n |-- extra: double (nullable = true)\n |-- mta_tax: double (nullable = true)\n |-- tip_amount: double (nullable = true)\n |-- tolls_amount: double (nullable = true)\n |-- improvement_surcharge: double (nullable = true)\n |-- congestion_surcharge: double (nullable = true)\n |-- Datayear: integer (nullable = false)\n |-- pickup_hour: integer (nullable = true)\n |-- pickup_month: integer (nullable = true)\n |-- pickup_dayofweek: integer (nullable = true)\n |-- Dur: integer (nullable = true)\n |-- speed: double (nullable = true)\n |-- pickup_weekday: integer (nullable = false)\n |-- season: integer (nullable = false)\n |-- long_trip: integer (nullable = false)\n |-- lfare_amount: double (nullable = true)\n |-- ltip_amount: double (nullable = true)\n |-- ltrip_distance: double (nullable = true)\n\n"}], "source": "full_df.printSchema()"}, {"cell_type": "code", "execution_count": 21, "id": "1c8d6ed6-96da-43e6-83fb-2ad503e7394d", "metadata": {}, "outputs": [], "source": "# Delete missing values and choose only those virtual values according to the official variable description\nfiltered_df = full_df.filter(col('VendorID').isin([1,2]))\nfiltered_df = filtered_df.filter(col('store_and_fwd_flag').isin(['N', 'Y']))\nfiltered_df = filtered_df.filter(col('RatecodeID').isin([1,2,3,4,5,6]))\nfiltered_df = filtered_df.filter(col('payment_type').isin([1,2,3,4,5]))"}, {"cell_type": "code", "execution_count": 22, "id": "1c18fcc8-2dc2-4809-b01a-763c78235a06", "metadata": {}, "outputs": [], "source": "# Convert store_and_fwd_flag to a dummy variable\nfiltered_df = filtered_df.withColumn(\"store_and_fwd_flag\", when(filtered_df.store_and_fwd_flag == \"Y\", 1).otherwise(0))"}, {"cell_type": "code", "execution_count": 23, "id": "14e0051c-0a9f-4e0c-86d7-54ac8fb0cd52", "metadata": {}, "outputs": [], "source": "# only save the log values\nfiltered_df = filtered_df.drop('trip_distance', 'fare_amount', 'tip_amount')"}, {"cell_type": "code", "execution_count": null, "id": "af03cccb-3eb5-47ef-8b7b-60c0ccee699f", "metadata": {}, "outputs": [], "source": "# Check missing values before one-hot encoding\n\n# Specify the categorical variables\ncategorical_vars = ['VendorID', 'store_and_fwd_flag', 'RatecodeID', 'PULocationID', 'DOLocationID', 'payment_type', 'season','pickup_dayofweek','Datayear']\n\n# Iterate over each categorical variable\nfor var in categorical_vars:\n    # Count the occurrences of each value\n    value_counts = filtered_df.groupBy(var).count().orderBy(col('count').desc())\n    \n    # Identify missing values\n    missing_count = filtered_df.filter(col(var).isNull() | (col(var) == '')).count()\n    \n    # Display the results\n    print(f\"Variable: {var}\")\n    value_counts.show()\n    print(f\"Missing values: {missing_count}\")\n    print()\n"}, {"cell_type": "code", "execution_count": 33, "id": "f80de71c-6a23-43c9-80b0-8ad5e8d13c26", "metadata": {}, "outputs": [], "source": "# drop_df = filtered_df.dropna()\n\n#for column in drop_df.columns:\n#    drop_df = drop_df.filter(col(column).isNotNull())"}, {"cell_type": "code", "execution_count": 24, "id": "941238d3-ba7f-41f1-ba7d-c293c40be2c7", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- VendorID: integer (nullable = true)\n |-- passenger_count: integer (nullable = true)\n |-- RatecodeID: integer (nullable = true)\n |-- store_and_fwd_flag: integer (nullable = false)\n |-- PULocationID: integer (nullable = true)\n |-- DOLocationID: integer (nullable = true)\n |-- payment_type: integer (nullable = true)\n |-- extra: double (nullable = true)\n |-- mta_tax: double (nullable = true)\n |-- tolls_amount: double (nullable = true)\n |-- improvement_surcharge: double (nullable = true)\n |-- congestion_surcharge: double (nullable = true)\n |-- Datayear: integer (nullable = false)\n |-- pickup_hour: integer (nullable = true)\n |-- pickup_month: integer (nullable = true)\n |-- pickup_dayofweek: integer (nullable = true)\n |-- Dur: integer (nullable = true)\n |-- speed: double (nullable = true)\n |-- pickup_weekday: integer (nullable = false)\n |-- season: integer (nullable = false)\n |-- long_trip: integer (nullable = false)\n |-- lfare_amount: double (nullable = true)\n |-- ltip_amount: double (nullable = true)\n |-- ltrip_distance: double (nullable = true)\n\n"}], "source": "filtered_df.printSchema()"}, {"cell_type": "markdown", "id": "7696c1f1-9933-47e3-9360-35c463a7c700", "metadata": {}, "source": "## Model"}, {"cell_type": "code", "execution_count": 28, "id": "e1f60346-068e-4760-9908-804b8f1bdc65", "metadata": {}, "outputs": [], "source": "# Split the data into training and testing sets\ntrain_data, test_data = filtered_df.randomSplit([0.7, 0.3], seed=27)"}, {"cell_type": "code", "execution_count": 29, "id": "1584e9f1-d3e3-4b29-bc67-6e59abc76597", "metadata": {}, "outputs": [], "source": "# One-hot encoding\ncategorical_columns = ['VendorID', 'RatecodeID', 'PULocationID', 'DOLocationID', 'payment_type',\n                       'season','pickup_dayofweek','Datayear'] # categorical columns\n\nindexers = [\n  StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c), handleInvalid=\"keep\")\n  for c in categorical_columns\n]\n\nencoders = [\n  OneHotEncoder(inputCol=indexer.getOutputCol(),\n                outputCol=\"{0}_encoded\".format(indexer.getOutputCol()))\n  for indexer in indexers\n]\n\nassemblerInputs = [encoder.getOutputCol() for encoder in encoders] + ['passenger_count','store_and_fwd_flag',\n                                                                      'extra', 'mta_tax', 'tolls_amount', 'improvement_surcharge', \n                                                                      'congestion_surcharge', 'pickup_hour', 'pickup_month',\n                                                                      'Dur', 'pickup_weekday', 'long_trip', \n                                                                      'ltip_amount', 'ltrip_distance', 'speed']\n\nvectorAssembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\", handleInvalid=\"keep\")\n"}, {"cell_type": "markdown", "id": "cc0a046f-6b22-4956-b869-12bc95c0be82", "metadata": {}, "source": "### Lasso"}, {"cell_type": "code", "execution_count": 30, "id": "69365ced-1c50-4b07-b22f-a5b646505895", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/05/19 00:52:07 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n23/05/19 00:52:07 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n23/05/19 00:52:08 ERROR breeze.optimize.OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n23/05/19 00:52:08 ERROR breeze.optimize.OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n23/05/19 00:52:08 ERROR breeze.optimize.OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n23/05/19 00:52:08 ERROR breeze.optimize.OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n23/05/19 00:52:08 ERROR breeze.optimize.OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n23/05/19 00:52:08 ERROR breeze.optimize.OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n23/05/19 00:52:08 ERROR breeze.optimize.OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n23/05/19 00:52:08 ERROR breeze.optimize.OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n23/05/19 00:52:08 ERROR breeze.optimize.OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n[Stage 66:>                                                       (0 + 20) / 64]\r"}, {"ename": "IllegalArgumentException", "evalue": "label does not exist. Available: VendorID, passenger_count, RatecodeID, store_and_fwd_flag, PULocationID, DOLocationID, payment_type, extra, mta_tax, tolls_amount, improvement_surcharge, congestion_surcharge, Datayear, pickup_hour, pickup_month, pickup_dayofweek, Dur, speed, pickup_weekday, season, long_trip, lfare_amount, ltip_amount, ltrip_distance, CrossValidator_e29d1c28e796_rand, VendorID_indexed, RatecodeID_indexed, PULocationID_indexed, DOLocationID_indexed, payment_type_indexed, season_indexed, pickup_dayofweek_indexed, Datayear_indexed, VendorID_indexed_encoded, RatecodeID_indexed_encoded, PULocationID_indexed_encoded, DOLocationID_indexed_encoded, payment_type_indexed_encoded, season_indexed_encoded, pickup_dayofweek_indexed_encoded, Datayear_indexed_encoded, features, prediction", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)", "Cell \u001b[0;32mIn[30], line 20\u001b[0m\n\u001b[1;32m     14\u001b[0m crossval_lasso \u001b[38;5;241m=\u001b[39m CrossValidator(estimator\u001b[38;5;241m=\u001b[39mpipeline_lasso,\n\u001b[1;32m     15\u001b[0m                           estimatorParamMaps\u001b[38;5;241m=\u001b[39mparamGrid_lasso,\n\u001b[1;32m     16\u001b[0m                           evaluator\u001b[38;5;241m=\u001b[39mRegressionEvaluator(),\n\u001b[1;32m     17\u001b[0m                           numFolds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Run cross-validation, and choose the best set of parameters:\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m cvModel_lasso \u001b[38;5;241m=\u001b[39m \u001b[43mcrossval_lasso\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/base.py:161\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/tuning.py:687\u001b[0m, in \u001b[0;36mCrossValidator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    684\u001b[0m train \u001b[38;5;241m=\u001b[39m datasets[i][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcache()\n\u001b[1;32m    686\u001b[0m tasks \u001b[38;5;241m=\u001b[39m _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam)\n\u001b[0;32m--> 687\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: f(), tasks):\n\u001b[1;32m    688\u001b[0m     metrics[j] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (metric \u001b[38;5;241m/\u001b[39m nFolds)\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m collectSubModelsParam:\n", "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.8/multiprocessing/pool.py:868\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m    867\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[0;32m--> 868\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n", "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.8/multiprocessing/pool.py:125\u001b[0m, in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    123\u001b[0m job, i, func, args, kwds \u001b[38;5;241m=\u001b[39m task\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     result \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wrap_exception \u001b[38;5;129;01mand\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _helper_reraises_exception:\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/tuning.py:687\u001b[0m, in \u001b[0;36mCrossValidator._fit.<locals>.<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    684\u001b[0m train \u001b[38;5;241m=\u001b[39m datasets[i][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcache()\n\u001b[1;32m    686\u001b[0m tasks \u001b[38;5;241m=\u001b[39m _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam)\n\u001b[0;32m--> 687\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, tasks):\n\u001b[1;32m    688\u001b[0m     metrics[j] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (metric \u001b[38;5;241m/\u001b[39m nFolds)\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m collectSubModelsParam:\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/tuning.py:74\u001b[0m, in \u001b[0;36m_parallelFitTasks.<locals>.singleTask\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m index, model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(modelIter)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# TODO: duplicate evaluator to take extra params from input\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m#  Note: Supporting tuning params in evaluator need update method\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m#  `MetaAlgorithmReadWrite.getAllNestedStages`, make it return\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m#  all nested stages and evaluators\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m metric \u001b[38;5;241m=\u001b[39m \u001b[43meva\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalidation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepm\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index, metric, model \u001b[38;5;28;01mif\u001b[39;00m collectSubModel \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/evaluation.py:84\u001b[0m, in \u001b[0;36mEvaluator.evaluate\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_evaluate(dataset)\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/evaluation.py:120\u001b[0m, in \u001b[0;36mJavaEvaluator._evaluate\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03mEvaluates the output.\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    evaluation metric\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n", "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n", "\u001b[0;31mIllegalArgumentException\u001b[0m: label does not exist. Available: VendorID, passenger_count, RatecodeID, store_and_fwd_flag, PULocationID, DOLocationID, payment_type, extra, mta_tax, tolls_amount, improvement_surcharge, congestion_surcharge, Datayear, pickup_hour, pickup_month, pickup_dayofweek, Dur, speed, pickup_weekday, season, long_trip, lfare_amount, ltip_amount, ltrip_distance, CrossValidator_e29d1c28e796_rand, VendorID_indexed, RatecodeID_indexed, PULocationID_indexed, DOLocationID_indexed, payment_type_indexed, season_indexed, pickup_dayofweek_indexed, Datayear_indexed, VendorID_indexed_encoded, RatecodeID_indexed_encoded, PULocationID_indexed_encoded, DOLocationID_indexed_encoded, payment_type_indexed_encoded, season_indexed_encoded, pickup_dayofweek_indexed_encoded, Datayear_indexed_encoded, features, prediction"]}, {"name": "stderr", "output_type": "stream", "text": "23/05/19 00:55:20 ERROR breeze.optimize.OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n23/05/19 00:55:20 ERROR breeze.optimize.OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n23/05/19 00:55:20 ERROR breeze.optimize.OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n23/05/19 00:55:20 ERROR breeze.optimize.OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n23/05/19 00:55:20 ERROR breeze.optimize.OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n23/05/19 00:55:20 ERROR breeze.optimize.OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n23/05/19 00:55:20 ERROR breeze.optimize.OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n23/05/19 00:55:20 ERROR breeze.optimize.OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n23/05/19 00:55:20 ERROR breeze.optimize.OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n23/05/19 00:58:18 ERROR breeze.optimize.OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n23/05/19 00:58:18 ERROR breeze.optimize.OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n23/05/19 00:58:18 ERROR breeze.optimize.OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n23/05/19 00:58:18 ERROR breeze.optimize.OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n23/05/19 00:58:18 ERROR breeze.optimize.OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n23/05/19 00:58:18 ERROR breeze.optimize.OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n23/05/19 00:58:18 ERROR breeze.optimize.OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n23/05/19 00:58:18 ERROR breeze.optimize.OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n23/05/19 00:58:18 ERROR breeze.optimize.OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n23/05/19 01:00:48 ERROR breeze.optimize.OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n23/05/19 01:00:48 ERROR breeze.optimize.OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n23/05/19 01:00:48 ERROR breeze.optimize.OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n23/05/19 01:00:48 ERROR breeze.optimize.OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n23/05/19 01:00:48 ERROR breeze.optimize.OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n23/05/19 01:00:48 ERROR breeze.optimize.OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n23/05/19 01:00:48 ERROR breeze.optimize.OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n23/05/19 01:00:48 ERROR breeze.optimize.OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n23/05/19 01:00:48 ERROR breeze.optimize.OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n23/05/19 01:03:14 ERROR breeze.optimize.OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n23/05/19 01:03:14 ERROR breeze.optimize.OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n23/05/19 01:03:14 ERROR breeze.optimize.OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n23/05/19 01:03:14 ERROR breeze.optimize.OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n23/05/19 01:03:14 ERROR breeze.optimize.OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n23/05/19 01:03:14 ERROR breeze.optimize.OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n23/05/19 01:03:14 ERROR breeze.optimize.OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n23/05/19 01:03:14 ERROR breeze.optimize.OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n23/05/19 01:03:14 ERROR breeze.optimize.OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n23/05/19 01:04:30 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 63.0 in stage 270.0 (TID 4470) (hub-hub-msca-bdp-dphub-student-fyw-sw-t8cv.c.msca-bdp-student-ap.internal executor 17): TaskKilled (Stage cancelled)\n23/05/19 01:04:30 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 62.0 in stage 270.0 (TID 4469) (hub-hub-msca-bdp-dphub-student-fyw-sw-t8cv.c.msca-bdp-student-ap.internal executor 17): TaskKilled (Stage cancelled)\n23/05/19 01:04:30 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 28.0 in stage 270.0 (TID 4467) (hub-hub-msca-bdp-dphub-student-fyw-w-0.c.msca-bdp-student-ap.internal executor 13): TaskKilled (Stage cancelled)\n23/05/19 01:04:30 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 31.0 in stage 270.0 (TID 4464) (hub-hub-msca-bdp-dphub-student-fyw-w-0.c.msca-bdp-student-ap.internal executor 14): TaskKilled (Stage cancelled)\n23/05/19 01:04:30 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 32.0 in stage 270.0 (TID 4471) (hub-hub-msca-bdp-dphub-student-fyw-w-1.c.msca-bdp-student-ap.internal executor 5): TaskKilled (Stage cancelled)\n23/05/19 01:04:30 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 33.0 in stage 270.0 (TID 4472) (hub-hub-msca-bdp-dphub-student-fyw-w-1.c.msca-bdp-student-ap.internal executor 5): TaskKilled (Stage cancelled)\n23/05/19 01:04:30 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 61.0 in stage 270.0 (TID 4466) (hub-hub-msca-bdp-dphub-student-fyw-sw-t8cv.c.msca-bdp-student-ap.internal executor 16): TaskKilled (Stage cancelled)\n23/05/19 01:04:30 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 60.0 in stage 270.0 (TID 4465) (hub-hub-msca-bdp-dphub-student-fyw-sw-t8cv.c.msca-bdp-student-ap.internal executor 16): TaskKilled (Stage cancelled)\n23/05/19 01:04:30 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 29.0 in stage 270.0 (TID 4468) (hub-hub-msca-bdp-dphub-student-fyw-w-0.c.msca-bdp-student-ap.internal executor 13): TaskKilled (Stage cancelled)\n23/05/19 01:04:30 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 19.0 in stage 270.0 (TID 4463) (hub-hub-msca-bdp-dphub-student-fyw-sw-m2l3.c.msca-bdp-student-ap.internal executor 12): TaskKilled (Stage cancelled)\n23/05/19 01:04:30 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 37.0 in stage 270.0 (TID 4476) (hub-hub-msca-bdp-dphub-student-fyw-sw-m2l3.c.msca-bdp-student-ap.internal executor 11): TaskKilled (Stage cancelled)\n23/05/19 01:04:30 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 36.0 in stage 270.0 (TID 4474) (hub-hub-msca-bdp-dphub-student-fyw-sw-m2l3.c.msca-bdp-student-ap.internal executor 11): TaskKilled (Stage cancelled)\n23/05/19 01:04:30 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 34.0 in stage 270.0 (TID 4477) (hub-hub-msca-bdp-dphub-student-fyw-sw-m2l3.c.msca-bdp-student-ap.internal executor 12): TaskKilled (Stage cancelled)\n23/05/19 01:04:30 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 38.0 in stage 270.0 (TID 4473) (hub-hub-msca-bdp-dphub-student-fyw-w-1.c.msca-bdp-student-ap.internal executor 15): TaskKilled (Stage cancelled)\n23/05/19 01:04:30 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 39.0 in stage 270.0 (TID 4475) (hub-hub-msca-bdp-dphub-student-fyw-w-1.c.msca-bdp-student-ap.internal executor 15): TaskKilled (Stage cancelled)\n"}], "source": "# Define the Lasso model (Linear regression with L1 regularization is equivalent to Lasso)\nlasso = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=1, featuresCol=\"features\", labelCol=\"lfare_amount\")\n\n# Define a pipeline model\npipeline_lasso = Pipeline(stages=indexers + encoders + [vectorAssembler, lasso])\n\n# Define a grid of hyperparameters to test:\nparamGrid_lasso = ParamGridBuilder() \\\n    .addGrid(lasso.regParam, [0.1, 0.3, 0.5]) \\\n    .addGrid(lasso.elasticNetParam, [0.8, 1.0]) \\\n    .build()\n\n# Define cross-validation:\ncrossval_lasso = CrossValidator(estimator=pipeline_lasso,\n                          estimatorParamMaps=paramGrid_lasso,\n                          evaluator=RegressionEvaluator(),\n                          numFolds=5)\n\n# Run cross-validation, and choose the best set of parameters:\ncvModel_lasso = crossval_lasso.fit(train_data)\n"}, {"cell_type": "code", "execution_count": null, "id": "a1faa6a8-c7a4-4a4d-b0b4-777e1b811880", "metadata": {}, "outputs": [], "source": "# Make predictions on the test data\npredictions_lasso = cvModel_lasso.transform(test_data)\n"}, {"cell_type": "code", "execution_count": null, "id": "92e4b7df-d596-4845-8057-091a962774a7", "metadata": {}, "outputs": [], "source": "# Evaluate the Lasso model\nevaluator = RegressionEvaluator()\nrmse = evaluator.evaluate(predictions_lasso)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"}, {"cell_type": "code", "execution_count": null, "id": "27b99561-e07d-4883-aabd-8360066f39e8", "metadata": {}, "outputs": [], "source": "# Visualization\n\n# Extract the cross-validation results\ncvModel_lasso = cv_model.avgMetrics\n\n# Extract the regularization parameters from the parameter grid\nreg_params = [param[lasso.regParam] for param in param_grid]\n\n# Plot the cross-validation results\nplt.figure(figsize=(8, 6))\nplt.plot(reg_params, cv_results, marker='o')\nplt.xscale('log')\nplt.xlabel('Regularization Parameter')\nplt.ylabel('RMSE')\nplt.title('Cross-Validation Results for Lasso')\nplt.grid(True)\nplt.show()"}, {"cell_type": "markdown", "id": "dbd60d27-3c8f-4939-80f0-141150b2264d", "metadata": {}, "source": "### Ransom Forest"}, {"cell_type": "code", "execution_count": null, "id": "f7927763-f6ea-43f4-9620-f51434d9416f", "metadata": {}, "outputs": [], "source": "# Extract the features selected by Lasso\ncoefficients = cvModel_lasso.bestModel.stages[-1].coefficients\nselected_features = [assemblerInputs[i] for i in range(len(coefficients)) if coefficients[i] != 0]\n\n# Build a new VectorAssembler with the selected features\nvectorAssembler_rf = VectorAssembler(inputCols=selected_features, outputCol=\"features\")"}, {"cell_type": "code", "execution_count": 65, "id": "d5335de3-abe5-4aa6-9b0e-437849c2e631", "metadata": {}, "outputs": [{"ename": "IllegalArgumentException", "evalue": "passenger_count does not exist. Available: features, lfare_amount", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)", "Cell \u001b[0;32mIn[65], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline(stages\u001b[38;5;241m=\u001b[39m[assembler, cv])\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Fit the pipeline to the data\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[1;32m     30\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransform(test)\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/base.py:161\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/pipeline.py:112\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stage, Transformer):\n\u001b[1;32m    111\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(stage)\n\u001b[0;32m--> 112\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     model \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mfit(dataset)\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/base.py:217\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_transform(dataset)\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/wrapper.py:350\u001b[0m, in \u001b[0;36mJavaTransformer._transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset):\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 350\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m, dataset\u001b[38;5;241m.\u001b[39msql_ctx)\n", "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n", "\u001b[0;31mIllegalArgumentException\u001b[0m: passenger_count does not exist. Available: features, lfare_amount"]}], "source": "# Create a VectorAssembler to combine the features into a single feature vector\n#assembler = VectorAssembler(inputCols=[ ], outputCol='features') # Use feature selected from lasso\n\n# Create a Random Forest regressor\nrf = RandomForestRegressor(featuresCol='features', labelCol='lfare_amount')\n\n# Define the parameter grid for hyperparameter tuning\nparamGrid = ParamGridBuilder() \\\n    .addGrid(rf.numTrees, [50,500,1000]) \\\n    .addGrid(rf.maxDepth, [5, 10, 15]) \\\n    .addGrid(rf.minInfoGain, [0.0, 0.1, 0.2]) \\\n    .build()\n\n# Create the evaluator\nevaluator = RegressionEvaluator(labelCol='lfare_amount', predictionCol='prediction', metricName='rmse')\n\n# Create a CrossValidator\ncv = CrossValidator(estimator=rf,\n                    estimatorParamMaps=paramGrid,\n                    evaluator=evaluator,\n                    numFolds=5)  # Specify the number of folds for cross-validation\n\n# Create a pipeline\npipeline = Pipeline(stages=[assembler, cv])\n\n# Fit the pipeline to the data\nmodel = pipeline.fit(train)\n\n# Make predictions\npredictions = model.transform(test)\n\n# Evaluate the model using the evaluator\nrmse = evaluator.evaluate(predictions)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)"}, {"cell_type": "code", "execution_count": null, "id": "a2fc7c24-2937-4d26-9625-8a19d3096a42", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.15"}}, "nbformat": 4, "nbformat_minor": 5}